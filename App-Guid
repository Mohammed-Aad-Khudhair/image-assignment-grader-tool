ProjectGrader — User Manual (GUI-first)
1) The Interface (GUI / UI)
Top toolbar
•	Load Expected Image
Pick the teacher’s reference image for Single Evaluation (shown on the left preview).
•	Load Student Image
Pick one student image for Single Evaluation (shown on the right preview).
•	Select Expected Dir
Choose the folder that contains expected images (used for context/mapping and batch workflows).
•	Select Student Dir
Choose the parent folder containing all student subfolders (s1, s2, …). Enables Current Student.
•	Current Student
Dropdown listing subfolders in the Student Dir. Select a student to give the app context (especially for cout rules).
•	Refresh Students
Re-scan the Student Dir for new/removed subfolders.
•	Load Mask / ROI
Limit grading to a region.
• Yes → draw a rectangular ROI on the expected image (quick mask).
• No → load a binary mask image (white=included, black=ignored). The mask is resized to the expected image.
•	Clear Images
Clears both previews.
•	Reset All
Clears previews, the rules table, ROI/mask, and directory paths.
Rules table (center)
One row per rule with columns:
•	Output (student filename) — the expected student output filename (e.g., mask_stud.png) or cout for console text.
•	Metric — how to compare (SSIM, ΔE, IoU, etc. for images; exact / contains / regex for cout).
•	MaxScore — points for this rule.
•	Tolerance(s) — thresholds (for cout, paste the expected text/pattern here).
•	Mode — scoring: bucket (step bands) or continuous (smooth).
•	Aggregation — how to reduce per-pixel maps (mean / median / max / trim10). Used with map-metrics like ΔE/MSE/MAE.
•	Notes — advanced options (e.g., file=stdout.txt, logic=any, gray=1).
•	Result — shows “metric value | awarded score” after running.

Bottom bar
•	Add Rule (popup) / Delete Rule — manage rules.
•	Run Single Eval — evaluates the two loaded preview images (and any cout rules using Current Student).
•	Batch Evaluate (student dir) — grades all student subfolders using the currently loaded expected image; saves a CSV.
•	Import Rules (JSON) / Export Rules (JSON/ugly) — load/save your rubric.
•	Rules text (JSON list) — editable JSON view of all rules.
• Refresh Text from Table → dump the table to JSON.
• Apply Text to Table → parse JSON and rebuild the table.
•	Scan Assignment Folder — drafts rules by scanning code/files for *_stud.* images and console usage. It’s a starting point; edit as needed.
Quick workflows

Single Evaluation (one pair)
1.	Load Expected Image.
2.	Load Student Image.
3.	Add a rule (e.g., SSIM, tolerances 0.95,0.90,0.80).
4.	(Optional) Load Mask / ROI.
5.	Click Run Single Eval → check the Result column.
Batch (many students)
1.	Select Expected Dir and Student Dir.
2.	Load the expected image for this task.
3.	Add/import rules.
4.	Click Batch Evaluate, pick a CSV path.
5.	Repeat steps 2–4 for other expected images/tasks.
________________________________________
2) Rules — how to fill a row
•	Output: a student filename (*_stud.png) or cout.
•	Metric: choose the comparison (see Section 3).
•	MaxScore: points for the rule.
•	Tolerance(s): thresholds or (for cout) the exact text/snippet/regex to match.
•	Mode: bucket or continuous (Section 5).
•	Aggregation: only for map metrics (ΔE/MSE/MAE).
•	Notes: optional keywords that change behavior (Section 6).
•	Result: filled after you run Single/Batch evaluation.

Examples
•	Image/SSIM:
Metric=SSIM, Tolerance(s)=0.95,0.90,0.80, Mode=bucket, MaxScore=2.
•	Console (cout) exact:
Metric=exact, Tolerance(s)=full expected text, Notes=file=stdout.txt.
•	N_DIFF_PIXELS:
Tolerance(s)=thr:10 | 3000,8000,12000, Notes=gray=1 (optional).
________________________________________
3) Metrics — what, when to use, and formulas
Key idea: some metrics are higher is better (e.g., SSIM); others are lower is better (e.g., MSE). Tolerances should match that direction.
Image fidelity / error
•	EXACT — strict equality.
When: deterministic tasks that must match exactly.
Better: higher (1 = match).
Formula: EXACT(A,B) = 1 if A==B else 0.
•	MSE (Mean Squared Error) — average squared pixel error.
When: generic difference; punishes big errors heavily.
Better: lower.
Formula: MSE = (1/N) Σ (A−B)^2.
•	MAE (Mean Absolute Error) — average absolute pixel error.
When: robust to outliers vs MSE.
Better: lower.
Formula: MAE = (1/N) Σ |A−B|.
•	RMSE (Root MSE) — MSE in original units.
Better: lower.
Formula: RMSE = sqrt(MSE).
•	PSNR (Peak Signal-to-Noise Ratio) — standard for compression/denoise.
When: quality vs reference.
Better: higher.
Formula: PSNR = 20 * log10(255 / sqrt(MSE)).
•	SSIM (Structural Similarity Index) — perceptual/structural similarity.
When: blur/denoise tasks; human-aligned.
Better: higher (≈0–1).
Formula: standard SSIM(A,B) on grayscale.
•	NCC (Normalized Cross-Correlation) — pattern correlation.
When: template-matching style checks.
Better: higher (−1..1).
•	ΔE (CIEDE2000 color difference) — perceptual color error in Lab.
When: color-space tasks, color fidelity.
Better: lower.
Bands often used: 1,2,10,50.
Binary/shape


•	IoU (Intersection over Union) — overlap of two binary masks.
When: segmentation/thresholding correctness.
Better: higher.
Formula: IoU = |A ∩ B| / |A ∪ B|.
•	Objects — compare number of connected components.
When: morphological tasks expecting a count.
Better: higher if counts match (or score by closeness).
•	Holes — compare number of holes (components on inverted mask).
When: shapes with internal voids (rings, donuts).
Better: higher if counts match (or by closeness).
•	Contours — quick check: contour counts equal?
When: coarse boundary check.
Better: higher if counts match (1), else 0.
•	ContoursStrict — hierarchy-aware shape similarity (holes per object).
When: stricter boundary/topology comparison.
Better: higher (0..1).
Pixel-difference counter
•	N_DIFF_PIXELS — counts pixels whose difference exceeds a per-pixel threshold.
When: you want to “count mistakes”.
Better: lower (fewer bad pixels).
How to write:
o	Tolerance(s): thr:<τ> | C1,C2,C3...
e.g., thr:10 | 3000,8000,12000
• thr:10 → a pixel is “different” if abs-diff > 10 (per channel or gray).
• 3000,8000,12000 → count thresholds for scoring (lower is better).
o	Notes (optional):
• logic=any (default): a pixel counts different if any channel > τ.
• logic=all: counts different only if all channels > τ (stricter).
• gray=1: compare in grayscale (ignore color).





________________________________________
4) Aggregation (for per-pixel maps)
Used when a metric gives a value per pixel (ΔE, |A−B|). It reduces many values to one.
•	mean — average of all pixels.
mean = (1/N) Σ x_i
Use when the overall error matters.
•	median — middle value after sorting (robust).
Use when a few outliers shouldn’t dominate.
•	max — worst pixel.
Use when any severe local error must be penalized.
•	trim10 — drop lowest 10% and highest 10%, then mean.
Use when you want a stable mean that ignores extremes.
Aggregation is ignored for scalar metrics (SSIM/PSNR/IoU/etc.).
________________________________________

5) Tolerance(s) — thresholds & how to write them
Tolerances define acceptable ranges and drive the score.
•	Higher-is-better (SSIM, PSNR, NCC, IoU, Contours/Strict, EXACT):
Example 0.95,0.90,0.80
o	≥0.95 → best band
o	≥0.90 → next
o	≥0.80 → next
o	otherwise 0
•	Lower-is-better (MSE/MAE/RMSE, ΔE, N_DIFF_PIXELS):
Example 1,2,10,50
o	≤1 → best band
o	≤2 → next …
o	otherwise 0
Special (N_DIFF_PIXELS):
thr:10 | 3000,8000,12000
•	thr:10 → per-pixel sensitivity.
•	counts after | → scoring bands (fewer is better).



________________________________________
6) Scoring modes — bucket vs continuous
•	bucket (step bands)
Tolerances cut the scale into bands.
Easy to explain (“A/B/C/F” style).
•	continuous (smooth)
Score scales gradually with the value.
Fairer when results often fall between bands.
Simple formulas (intuition):
•	Higher-is-better: score ≈ min(value / top_threshold, 1) * MaxScore
•	Lower-is-better: score ≈ max(0, 1 − value / largest_tolerance) * MaxScore
________________________________________
7) Keywords / Notes (advanced options)
These are commands (not just comments) that alter behavior:
•	file=stdout.txt — for cout rules: which file to read (default stdout.txt).
•	logic=any / logic=all — for N_DIFF_PIXELS per-pixel decision:
• any → a pixel is bad if any channel exceeds τ (looser; default).
• all → a pixel is bad only if all channels exceed τ (stricter).
•	gray=1 — for N_DIFF_PIXELS: compare in grayscale (ignore color).
You can leave Notes empty unless you want these behaviors.
________________________________________
8) ROI / Mask
•	Load Mask / ROI →
• Yes to draw a rectangle ROI quickly.
• No to load a binary mask image (white=included).
•	Only masked pixels contribute to metrics.
•	The mask is resized to the expected image automatically.
Why it matters: focus grading on the important region; ignore background/borders.



________________________________________
9) Glossary (abbreviations)
•	GUI / UI — Graphical User Interface.
•	JSON — JavaScript Object Notation (text format for rule sets).
•	CSV — Comma-Separated Values (spreadsheet-friendly results).
•	cout — Console output (text printed by a program).
•	regex — Regular expression (text pattern; e.g., ^Result: \d+$).
•	ROI — Region of Interest (the graded part of the image).
•	SSIM — Structural Similarity Index (visual similarity).
•	PSNR — Peak Signal-to-Noise Ratio (quality; higher is better).
•	NCC — Normalized Cross-Correlation (pattern correlation).
•	IoU — Intersection over Union (mask overlap).
•	ΔE (CIEDE2000) — perceptual color difference in Lab.
•	MSE/MAE/RMSE — error measures (mean squared/absolute/root mean squared).
•	Aggregation — how per-pixel maps are summarized (mean/median/max/trim10).
•	Tolerance(s) — thresholds defining acceptable ranges & bands.
•	Bucket / Continuous — step-based vs smooth scoring.
________________________________________
10) Quick recipes
•	Exact mask
Metric=EXACT, MaxScore=1, Mode=bucket, Tolerances=—.
•	SSIM quality
Metric=SSIM, Tolerances=0.95,0.90,0.80, Mode=bucket.
•	ΔE color fidelity
Metric=ΔE, Tolerances=1,2,10,50, Mode=continuous, Aggregation=mean.
•	IoU segmentation
Metric=IoU, Tolerances=0.95,0.90,0.80, Mode=bucket.
•	N_DIFF_PIXELS
Tolerance(s)=thr:10 | 3000,8000,12000, Notes=gray=1 (optional), Mode=bucket.
•	Console exact
Output=cout, Metric=exact, Tolerance(s)=full expected text, 
•	Notes=file=stdout.txt.
________________________________________
11) Troubleshooting
•	“No images loaded” → load both Expected & Student images before Single Eval.
•	“No student dir/image for cout” → set Student Dir and pick a Current Student.
•	cout exact fails → paste the exact expected text (including newlines) into Tolerance(s); set file=… if needed.
•	ROI has no effect → draw/load it, then re-run; mask is applied during evaluation.
•	Batch for multiple tasks → run once per expected image (load expected → Batch → repeat).
